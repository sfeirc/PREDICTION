"""
ðŸ¤– TRANSFORMER MODEL FOR TRADING

Implements a state-of-the-art Transformer with:
- Multi-head attention
- Positional encoding
- Layer normalization
- Dropout regularization

Expected: +10-15% accuracy improvement
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Optional, Tuple

logger = logging.getLogger(__name__)


class PositionalEncoding(nn.Module):
    """
    Positional encoding for Transformer.
    
    Adds position information to the sequence since Transformers
    don't have inherent notion of order.
    """
    
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor of shape [seq_len, batch_size, d_model]
        """
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)


class TradingTransformer(nn.Module):
    """
    Transformer model for trading prediction.
    
    Architecture:
    - Input projection
    - Positional encoding
    - N Transformer encoder layers
    - Classification head
    """
    
    def __init__(
        self,
        input_dim: int,
        d_model: int = 256,
        nhead: int = 8,
        num_layers: int = 6,
        dim_feedforward: int = 1024,
        dropout: float = 0.2,
        sequence_length: int = 60
    ):
        super().__init__()
        
        self.d_model = d_model
        self.sequence_length = sequence_length
        
        # Input projection
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # Positional encoding
        self.pos_encoder = PositionalEncoding(d_model, max_len=sequence_length, dropout=dropout)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation='gelu',
            batch_first=False  # [seq_len, batch, features]
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
            norm=nn.LayerNorm(d_model)
        )
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)  # Binary classification
        )
        
        # Multi-head attention for interpretability
        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        
        self._reset_parameters()
    
    def _reset_parameters(self):
        """Initialize parameters"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(
        self,
        x: torch.Tensor,
        return_attention: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Forward pass.
        
        Args:
            x: Input tensor of shape [batch_size, seq_len, input_dim]
            return_attention: If True, also return attention weights
        
        Returns:
            logits: [batch_size, 2]
            attention_weights: [batch_size, seq_len, seq_len] (optional)
        """
        # x: [batch, seq, features]
        
        # Project input
        x = self.input_projection(x)  # [batch, seq, d_model]
        
        # Transpose for transformer (needs [seq, batch, features])
        x = x.transpose(0, 1)  # [seq, batch, d_model]
        
        # Add positional encoding
        x = self.pos_encoder(x)
        
        # Transformer encoder
        encoded = self.transformer_encoder(x)  # [seq, batch, d_model]
        
        # Get attention weights if requested
        attention_weights = None
        if return_attention:
            attn_output, attention_weights = self.attention(encoded, encoded, encoded)
        
        # Use last timestep for classification (or could use mean/max pooling)
        last_hidden = encoded[-1]  # [batch, d_model]
        
        # Classification
        logits = self.classifier(last_hidden)  # [batch, 2]
        
        return logits, attention_weights


class TransformerTrainer:
    """
    Trainer for the Transformer model.
    """
    
    def __init__(
        self,
        model: TradingTransformer,
        learning_rate: float = 0.0001,
        weight_decay: float = 0.01,
        device: str = 'cpu'
    ):
        self.model = model.to(device)
        self.device = device
        
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True
        )
        
        self.criterion = nn.CrossEntropyLoss()
        
        logger.info(f"ðŸ¤– Transformer initialized on {device}")
        logger.info(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    def create_sequences(
        self,
        X: np.ndarray,
        y: np.ndarray,
        sequence_length: int = 60
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Create sequences from data.
        
        Args:
            X: Features [n_samples, n_features]
            y: Labels [n_samples]
            sequence_length: Length of each sequence
        
        Returns:
            X_seq: [n_sequences, seq_len, n_features]
            y_seq: [n_sequences]
        """
        n_samples, n_features = X.shape
        n_sequences = n_samples - sequence_length + 1
        
        X_seq = np.zeros((n_sequences, sequence_length, n_features))
        y_seq = np.zeros(n_sequences)
        
        for i in range(n_sequences):
            X_seq[i] = X[i:i+sequence_length]
            y_seq[i] = y[i+sequence_length-1]  # Predict for last timestep
        
        return torch.FloatTensor(X_seq), torch.LongTensor(y_seq)
    
    def train_epoch(
        self,
        X_train: torch.Tensor,
        y_train: torch.Tensor,
        batch_size: int = 32
    ) -> float:
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        n_batches = 0
        
        # Create batches
        n_samples = len(X_train)
        indices = torch.randperm(n_samples)
        
        for i in range(0, n_samples, batch_size):
            batch_indices = indices[i:i+batch_size]
            X_batch = X_train[batch_indices].to(self.device)
            y_batch = y_train[batch_indices].to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            logits, _ = self.model(X_batch)
            loss = self.criterion(logits, y_batch)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        return total_loss / n_batches
    
    def validate(
        self,
        X_val: torch.Tensor,
        y_val: torch.Tensor,
        batch_size: int = 32
    ) -> Tuple[float, float]:
        """Validate the model"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for i in range(0, len(X_val), batch_size):
                X_batch = X_val[i:i+batch_size].to(self.device)
                y_batch = y_val[i:i+batch_size].to(self.device)
                
                logits, _ = self.model(X_batch)
                loss = self.criterion(logits, y_batch)
                
                total_loss += loss.item()
                
                predictions = torch.argmax(logits, dim=1)
                correct += (predictions == y_batch).sum().item()
                total += len(y_batch)
        
        avg_loss = total_loss / (len(X_val) / batch_size)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray,
        epochs: int = 50,
        batch_size: int = 32,
        early_stopping_patience: int = 10
    ):
        """
        Train the model with early stopping.
        """
        logger.info(f"ðŸš€ Starting Transformer training...")
        
        # Create sequences
        X_train_seq, y_train_seq = self.create_sequences(
            X_train, y_train, self.model.sequence_length
        )
        X_val_seq, y_val_seq = self.create_sequences(
            X_val, y_val, self.model.sequence_length
        )
        
        logger.info(f"   Train sequences: {len(X_train_seq)}")
        logger.info(f"   Val sequences: {len(X_val_seq)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # Train
            train_loss = self.train_epoch(X_train_seq, y_train_seq, batch_size)
            
            # Validate
            val_loss, val_acc = self.validate(X_val_seq, y_val_seq, batch_size)
            
            # Update learning rate
            self.scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'models/transformer_best.pt')
            else:
                patience_counter += 1
            
            if (epoch + 1) % 5 == 0:
                logger.info(
                    f"   Epoch {epoch+1}/{epochs} - "
                    f"Train Loss: {train_loss:.4f}, "
                    f"Val Loss: {val_loss:.4f}, "
                    f"Val Acc: {val_acc:.2%}"
                )
            
            if patience_counter >= early_stopping_patience:
                logger.info(f"   Early stopping at epoch {epoch+1}")
                break
        
        # Load best model
        self.model.load_state_dict(torch.load('models/transformer_best.pt'))
        logger.info(f"âœ… Training complete! Best val loss: {best_val_loss:.4f}")
    
    def predict(
        self,
        X: np.ndarray,
        return_proba: bool = True,
        batch_size: int = 32
    ) -> np.ndarray:
        """Make predictions"""
        self.model.eval()
        
        # Create sequences
        X_seq, _ = self.create_sequences(X, np.zeros(len(X)), self.model.sequence_length)
        
        predictions = []
        
        with torch.no_grad():
            for i in range(0, len(X_seq), batch_size):
                X_batch = X_seq[i:i+batch_size].to(self.device)
                logits, _ = self.model(X_batch)
                
                if return_proba:
                    proba = torch.softmax(logits, dim=1)
                    predictions.append(proba.cpu().numpy())
                else:
                    preds = torch.argmax(logits, dim=1)
                    predictions.append(preds.cpu().numpy())
        
        return np.concatenate(predictions, axis=0)
    
    def get_attention_weights(
        self,
        X: np.ndarray,
        sample_idx: int = 0
    ) -> np.ndarray:
        """
        Get attention weights for interpretability.
        
        Shows what the model is focusing on.
        """
        self.model.eval()
        
        X_seq, _ = self.create_sequences(X, np.zeros(len(X)), self.model.sequence_length)
        X_sample = X_seq[sample_idx:sample_idx+1].to(self.device)
        
        with torch.no_grad():
            _, attention_weights = self.model(X_sample, return_attention=True)
        
        return attention_weights.cpu().numpy()


def test_transformer():
    """Test the Transformer model"""
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    logging.basicConfig(level=logging.INFO)
    
    print("="*80)
    print("ðŸ¤– TESTING TRANSFORMER MODEL")
    print("="*80)
    
    # Create dummy data
    n_samples = 1000
    n_features = 50
    
    X_train = np.random.randn(n_samples, n_features).astype(np.float32)
    y_train = np.random.randint(0, 2, n_samples)
    
    X_val = np.random.randn(200, n_features).astype(np.float32)
    y_val = np.random.randint(0, 2, 200)
    
    # Create model
    model = TradingTransformer(
        input_dim=n_features,
        d_model=128,
        nhead=4,
        num_layers=3,
        sequence_length=60
    )
    
    # Create trainer
    trainer = TransformerTrainer(model, learning_rate=0.001)
    
    # Train
    print("\nðŸš€ Training...")
    trainer.train(X_train, y_train, X_val, y_val, epochs=10, batch_size=16)
    
    # Predict
    print("\nðŸ“Š Making predictions...")
    predictions = trainer.predict(X_val, return_proba=True)
    print(f"   Predictions shape: {predictions.shape}")
    print(f"   Sample probabilities: {predictions[0]}")
    
    print("\nâœ… Transformer test complete!")
    print("="*80)


if __name__ == "__main__":
    test_transformer()

